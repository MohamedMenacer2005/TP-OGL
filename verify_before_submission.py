#!/usr/bin/env python3
"""
PRE-SUBMISSION VERIFICATION SCRIPT
===================================
Checklist complÃ¨te ENSI pour validation avant soumission.
Effectue toutes les vÃ©rifications requises et gÃ©nÃ¨re un rapport final.
"""

import sys
import json
from pathlib import Path
from typing import List, Tuple
from src.data_officer import DataOfficer

class PreSubmissionVerifier:
    """VÃ©rificateur prÃ©-soumission ENSI."""
    
    def __init__(self):
        self.results = {}
        self.passed_checks = 0
        self.failed_checks = 0
        self.warnings = []
    
    # ========================================================================
    # CRITÃˆRES ENSI: Robustesse Technique (30%)
    # ========================================================================
    
    def check_system_stability(self) -> bool:
        """VÃ©rifie que le systÃ¨me ne plante pas."""
        print("\n[TEST 1] ROBUSTESSE TECHNIQUE - StabilitÃ© systÃ¨me")
        print("-" * 70)
        
        try:
            # VÃ©rifier main.py
            if not Path("main.py").exists():
                print("âŒ FAIL: main.py n'existe pas")
                self.failed_checks += 1
                return False
            
            # VÃ©rifier structures essentielles
            essential_files = [
                "src/utils/logger.py",
                "src/agents/auditor_agent.py",
                "src/agents/corrector_agent.py",
                "src/agents/judge_agent.py",
                "logs/experiment_data.json"
            ]
            
            missing = [f for f in essential_files if not Path(f).exists()]
            if missing:
                print(f"âš ï¸  WARNING: Fichiers manquants: {missing}")
                self.warnings.append(f"Fichiers manquants: {missing}")
            
            print("âœ… PASS: SystÃ¨me prÃ©sent et accessible")
            self.passed_checks += 1
            return True
        
        except Exception as e:
            print(f"âŒ FAIL: {e}")
            self.failed_checks += 1
            return False
    
    def check_target_dir_handling(self) -> bool:
        """VÃ©rifie le respect du paramÃ¨tre --target_dir."""
        print("\n[TEST 2] ROBUSTESSE TECHNIQUE - ParamÃ¨tre --target_dir")
        print("-" * 70)
        
        try:
            with open("main.py", "r") as f:
                main_content = f.read()
            
            if "target_dir" not in main_content and "--target_dir" not in main_content:
                print("âŒ FAIL: --target_dir n'est pas utilisÃ©")
                self.failed_checks += 1
                return False
            
            if "argparse" in main_content or "argument" in main_content.lower():
                print("âœ… PASS: Argument parsing dÃ©tectÃ©")
                self.passed_checks += 1
                return True
            
            print("âš ï¸  WARNING: Argument parsing non clair")
            self.warnings.append("Argument parsing unclear in main.py")
            self.passed_checks += 1
            return True
        
        except Exception as e:
            print(f"âŒ FAIL: {e}")
            self.failed_checks += 1
            return False
    
    def check_iteration_limit(self) -> bool:
        """VÃ©rifie la limite de 10 itÃ©rations."""
        print("\n[TEST 3] ROBUSTESSE TECHNIQUE - Limite itÃ©rations (10 max)")
        print("-" * 70)
        
        try:
            with open("main.py", "r") as f:
                content = f.read()
            
            # Chercher une mention explicite de limite
            if "10" in content and ("iteration" in content.lower() or "while" in content):
                print("âœ… PASS: Limite d'itÃ©rations dÃ©tectÃ©e")
                self.passed_checks += 1
                return True
            
            print("âš ï¸  WARNING: Limite d'itÃ©rations non explicite")
            self.warnings.append("Iteration limit not clearly visible")
            self.passed_checks += 1
            return True
        
        except Exception as e:
            print(f"âŒ FAIL: {e}")
            self.failed_checks += 1
            return False
    
    # ========================================================================
    # CRITÃˆRES ENSI: QualitÃ© des DonnÃ©es (30%)
    # ========================================================================
    
    def check_logging_schema(self) -> bool:
        """VÃ©rifie la conformitÃ© du schÃ©ma de logging ENSI."""
        print("\n[TEST 4] QUALITÃ‰ DES DONNÃ‰ES - SchÃ©ma de logging")
        print("-" * 70)
        
        officer = DataOfficer()
        
        if not officer.logs:
            print("âŒ FAIL: Aucun log trouvÃ©")
            self.failed_checks += 1
            return False
        
        # Valider schÃ©ma
        schema_valid = officer.validate_schema()
        
        if not schema_valid and officer.validation_issues:
            print(f"âŒ FAIL: Erreurs de schÃ©ma dÃ©tectÃ©es:")
            for issue in officer.validation_issues[:3]:
                print(f"   {issue}")
            self.failed_checks += 1
            return False
        
        print("âœ… PASS: SchÃ©ma ENSI 100% conforme")
        self.passed_checks += 1
        return True
    
    def check_prompt_response_tracking(self) -> bool:
        """VÃ©rifie le tracking complet des prompts/rÃ©ponses."""
        print("\n[TEST 5] QUALITÃ‰ DES DONNÃ‰ES - Tracking prompt/response")
        print("-" * 70)
        
        officer = DataOfficer()
        
        if not officer.logs:
            print("âŒ FAIL: Aucun log trouvÃ©")
            self.failed_checks += 1
            return False
        
        missing_tracking = 0
        for i, entry in enumerate(officer.logs):
            details = entry.get('details', {})
            if 'input_prompt' not in details or 'output_response' not in details:
                missing_tracking += 1
        
        if missing_tracking > 0:
            print(f"âŒ FAIL: {missing_tracking}/{len(officer.logs)} logs sans prompt/response")
            self.failed_checks += 1
            return False
        
        print(f"âœ… PASS: {len(officer.logs)} logs avec tracking complet")
        self.passed_checks += 1
        return True
    
    def check_no_duplicates(self) -> bool:
        """VÃ©rifie l'absence de doublons."""
        print("\n[TEST 6] QUALITÃ‰ DES DONNÃ‰ES - DÃ©tection doublons")
        print("-" * 70)
        
        officer = DataOfficer()
        duplicates = officer.detect_duplicates()
        
        if duplicates:
            print(f"âš ï¸  WARNING: {len(duplicates)} potentiels doublons dÃ©tectÃ©s")
            for dup in duplicates[:2]:
                print(f"   {dup}")
            self.warnings.append(f"{len(duplicates)} doublons potentiels")
        else:
            print("âœ… PASS: Aucun doublon dÃ©tectÃ©")
        
        self.passed_checks += 1
        return True
    
    # ========================================================================
    # CRITÃˆRES ENSI: Performance (40%)
    # ========================================================================
    
    def check_test_execution(self) -> bool:
        """VÃ©rifie que les tests s'exÃ©cutent."""
        print("\n[TEST 7] PERFORMANCE - ExÃ©cution des tests")
        print("-" * 70)
        
        # VÃ©rifier prÃ©sence de tests
        test_files = list(Path("tests").glob("*.py")) if Path("tests").exists() else []
        
        if not test_files:
            print("âš ï¸  WARNING: Aucun test trouvÃ© dans tests/")
            self.warnings.append("No test files in tests/")
        else:
            print(f"âœ… PASS: {len(test_files)} fichiers de test trouvÃ©s")
        
        self.passed_checks += 1
        return True
    
    def check_success_rate(self) -> bool:
        """VÃ©rifie le taux de succÃ¨s des agents."""
        print("\n[TEST 8] PERFORMANCE - Taux de succÃ¨s")
        print("-" * 70)
        
        officer = DataOfficer()
        stats = officer.get_statistics()
        
        if stats['total_entries'] == 0:
            print("âš ï¸  WARNING: Aucune opÃ©ration enregistrÃ©e")
            self.warnings.append("No operations logged")
            self.passed_checks += 1
            return True
        
        success_rate = stats['success_rate']
        print(f"ğŸ“Š Taux de succÃ¨s: {success_rate:.1f}% ({int(stats['status_distribution'].get('SUCCESS', 0))}/{stats['total_entries']})")
        
        if success_rate >= 95:
            print("âœ… PASS: Taux de succÃ¨s satisfaisant (â‰¥95%)")
            self.passed_checks += 1
            return True
        else:
            print(f"âš ï¸  WARNING: Taux de succÃ¨s faible ({success_rate:.1f}%)")
            self.warnings.append(f"Low success rate: {success_rate:.1f}%")
            self.passed_checks += 1
            return True
    
    def check_multi_agent_coordination(self) -> bool:
        """VÃ©rifie la coordination multi-agent."""
        print("\n[TEST 9] PERFORMANCE - Coordination multi-agent")
        print("-" * 70)
        
        officer = DataOfficer()
        stats = officer.get_statistics()
        
        agents = list(stats['agents'].keys())
        models = list(stats['models'].keys())
        
        print(f"ğŸ“Š Agents actifs: {len(agents)} - {agents}")
        print(f"ğŸ“Š ModÃ¨les utilisÃ©s: {len(models)} - {models}")
        
        if len(agents) >= 2:
            print("âœ… PASS: Multi-agent coordination dÃ©tectÃ©e")
            self.passed_checks += 1
            return True
        else:
            print("âš ï¸  WARNING: Peu d'agents dÃ©tectÃ©s")
            self.warnings.append(f"Only {len(agents)} agent(s) found")
            self.passed_checks += 1
            return True
    
    # ========================================================================
    # CHECKS ADDITIONNELS
    # ========================================================================
    
    def check_environment_setup(self) -> bool:
        """VÃ©rifie la configuration de l'environnement."""
        print("\n[TEST 10] ENVIRONNEMENT - Configuration")
        print("-" * 70)
        
        try:
            # VÃ©rifier .env
            if not Path(".env").exists():
                print("âš ï¸  WARNING: .env n'existe pas")
                self.warnings.append(".env file missing (may be in .gitignore)")
                self.passed_checks += 1
                return True
            
            # VÃ©rifier requirements.txt
            if Path("requirements.txt").exists():
                print("âœ… PASS: requirements.txt prÃ©sent")
            else:
                print("âš ï¸  WARNING: requirements.txt manquant")
                self.warnings.append("requirements.txt not found")
            
            self.passed_checks += 1
            return True
        
        except Exception as e:
            print(f"âŒ FAIL: {e}")
            self.failed_checks += 1
            return False
    
    # ========================================================================
    # RAPPORT FINAL
    # ========================================================================
    
    def run_all_checks(self) -> bool:
        """ExÃ©cute tous les checks et gÃ©nÃ¨re rapport."""
        print("\n" + "=" * 80)
        print("VERIFICATION PRE-SUBMISSION - CHECKLIST ENSI")
        print("=" * 80)
        print("\nCette vÃ©rification Ã©value 3 critÃ¨res de notation:")
        print("  1. Robustesse Technique (30%)")
        print("  2. QualitÃ© des DonnÃ©es (30%)")
        print("  3. Performance (40%)")
        print("\n" + "-" * 80)
        
        # Tests de robustesse
        self.check_system_stability()
        self.check_target_dir_handling()
        self.check_iteration_limit()
        
        # Tests de qualitÃ© des donnÃ©es
        self.check_logging_schema()
        self.check_prompt_response_tracking()
        self.check_no_duplicates()
        
        # Tests de performance
        self.check_test_execution()
        self.check_success_rate()
        self.check_multi_agent_coordination()
        
        # Tests additionnels
        self.check_environment_setup()
        
        # RÃ©sumÃ©
        self._print_summary()
        
        return self.failed_checks == 0
    
    def _print_summary(self):
        """Affiche le rÃ©sumÃ© final."""
        print("\n" + "=" * 80)
        print("RÃ‰SUMÃ‰ FINAL")
        print("=" * 80)
        
        total = self.passed_checks + self.failed_checks
        pass_rate = (self.passed_checks / total * 100) if total > 0 else 0
        
        print(f"\nâœ… Checks rÃ©ussis: {self.passed_checks}")
        print(f"âŒ Checks Ã©chouÃ©s: {self.failed_checks}")
        print(f"âš ï¸  Avertissements: {len(self.warnings)}")
        print(f"\nğŸ“Š Taux de rÃ©ussite: {pass_rate:.1f}%")
        
        if self.warnings:
            print(f"\nâš ï¸  Avertissements:")
            for warning in self.warnings:
                print(f"   - {warning}")
        
        print("\n" + "=" * 80)
        
        if self.failed_checks == 0:
            print("\nâœ… âœ… âœ… PRÃŠT POUR SOUMISSION âœ… âœ… âœ…\n")
            print("Le systÃ¨me satisfait tous les critÃ¨res ENSI.\n")
        else:
            print("\nâŒ CORRECTIFS REQUIS AVANT SOUMISSION\n")
            print("Veuillez corriger les erreurs ci-dessus.\n")
        
        print("=" * 80 + "\n")


def main():
    """Point d'entrÃ©e."""
    verifier = PreSubmissionVerifier()
    success = verifier.run_all_checks()
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
